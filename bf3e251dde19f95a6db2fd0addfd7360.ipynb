{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a4f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guoqizhi/anaconda3/envs/fintune-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Load embedding model\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "embedding_model = BGEM3FlagModel('/data/aicuserData/guoqizhi/embedding_model/bge-m3',  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "import numpy\n",
    "Query_embedding = numpy.load('query_embedding.npy')\n",
    "Answer_embedding = numpy.load('answer_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b7f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching all file under given path\n",
    "import os\n",
    "def get_filepath_list(root_dir):\n",
    "    file_list = []\n",
    "    folder_list = []\n",
    "    \n",
    "    for root,dirs,files in os.walk(root_dir):\n",
    "        for i_file in files:\n",
    "            file_list.append(f'{root}/{i_file}')\n",
    "            \n",
    "        for i_dir in dirs:\n",
    "            folder_list.append(f'{root}/{i_dir}')\n",
    "    return file_list, folder_list\n",
    "\n",
    "# Classify file type\n",
    "def analyse_file_type(f_list):\n",
    "    file_type_list = {}\n",
    "    for file in f_list:\n",
    "        file_type = file.split('.')[-1]\n",
    "        if file_type in file_type_list.keys():\n",
    "            file_type_list[file_type].append(file)\n",
    "        else:\n",
    "            file_type_list[file_type] = [file]\n",
    "            \n",
    "    return file_type_list\n",
    "\n",
    "dir = '/home/guoqizhi/deepseek-chat/'\n",
    "file_list, folder_list = get_filepath_list(dir)\n",
    "file_type_list = analyse_file_type(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c63252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge different data\n",
    "from transformers import AutoTokenizer\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained('/data/aicuserData/guoqizhi/embedding_model/bge-m3')\n",
    "\n",
    "import json\n",
    "qa_dataset = []\n",
    "passage_dataset = []\n",
    "i = 0\n",
    "for file in file_type_list['json']:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        for d in data:\n",
    "            d['file'] = file.split('/')[-1]\n",
    "            d['block_id'] = i\n",
    "            i += 1\n",
    "            # Filte extrame long query and answer\n",
    "            for j in range(len(d['qa_pairs'])):\n",
    "                encoding_result = embedding_tokenizer(d['qa_pairs'][j]['Gura'], add_special_tokens=False)['input_ids']\n",
    "                if len(encoding_result) > 256:\n",
    "                    d['qa_pairs'][j]['Gura'] = embedding_tokenizer.decode(encoding_result[:512])\n",
    "            qa_dataset += d['qa_pairs']\n",
    "            passage_dataset.append(d['script'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d9b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embedding result\n",
    "import numpy\n",
    "Query = [d['User'] for d in qa_dataset]\n",
    "Answer = [d['Gura'] for d in qa_dataset]\n",
    "\n",
    "Query_embedding = numpy.load('query_embedding.npy')\n",
    "Answer_embedding = numpy.load('answer_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56b71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = [\n",
    "    'What is your favorite food?',\n",
    "    'Can you introduce yourself? Tell me about your name, age and hobbies. ',\n",
    "    'Do you remember when your debut took place? What is the exact date and time?',\n",
    "    'Can you give 3 shark facts that you mentioned in your stream?',\n",
    "    'What do you call your fans?',\n",
    "    'Which of the following is not your nickname? a) gooba b)goomba c)goob d)goola',\n",
    "    'Which of the following are you bad at?  a)English b)Mathematics c)Video games d)Drawing',\n",
    "    'You only speak English and Chinese, but not Japanese, is that correct? ',\n",
    "    'Which Vtuber agency do you belong to?  a) Hololive  b)Nijisanji c)VShojo d)Youtube',\n",
    "    'What is your favorite type of shark?',\n",
    "    'Do you know who Watson Amelia is? If yes,  tell me more about your relationship with her.',\n",
    "    'What is the name of your first original song?',\n",
    "    'Can you describe your outfit and its unique features?',\n",
    "    'What games do you often play on your streams?',\n",
    "    'Have you ever collaborated with other Hololive members? If so, who were they?',\n",
    "    'How tall are you?',\n",
    "    'When is your birthday?',\n",
    "    'How old are you?',\n",
    "]\n",
    "\n",
    "user_input_embedding = embedding_model.encode(user_input, return_dense=True, return_sparse=True, return_colbert_vecs=False)['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab632612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load rank model\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n",
    "    if prompt is None:\n",
    "        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n",
    "    sep = \"\\n\"\n",
    "    prompt_inputs = tokenizer(prompt,\n",
    "                              return_tensors=None,\n",
    "                              add_special_tokens=False)['input_ids']\n",
    "    sep_inputs = tokenizer(sep,\n",
    "                           return_tensors=None,\n",
    "                           add_special_tokens=False)['input_ids']\n",
    "    inputs = []\n",
    "    for query, passage in pairs:\n",
    "        query_inputs = tokenizer(f'A: {query}',\n",
    "                                 return_tensors=None,\n",
    "                                 add_special_tokens=False,\n",
    "                                 max_length=max_length,# * 3 // 4,\n",
    "                                 truncation=True)\n",
    "        passage_inputs = tokenizer(f'B: {passage}',\n",
    "                                   return_tensors=None,\n",
    "                                   add_special_tokens=False,\n",
    "                                   max_length=max_length,\n",
    "                                   truncation=True)\n",
    "        item = tokenizer.prepare_for_model(\n",
    "            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n",
    "            sep_inputs + passage_inputs['input_ids'],\n",
    "            truncation='only_second',\n",
    "            max_length=max_length,\n",
    "            padding=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n",
    "        item['attention_mask'] = [1] * len(item['input_ids'])\n",
    "        inputs.append(item)\n",
    "    return tokenizer.pad(\n",
    "            inputs,\n",
    "            padding=True,\n",
    "            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n",
    "            pad_to_multiple_of=8,\n",
    "            return_tensors='pt',\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data/aicuserData/guoqizhi/embedding_model/bge-reranker-v2-gemma/')\n",
    "model = AutoModelForCausalLM.from_pretrained('/data/aicuserData/guoqizhi/embedding_model/bge-reranker-v2-gemma/').half().cuda()\n",
    "yes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d1417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rank score\n",
    "def compute_score(tokenizer, model, match_list):\n",
    "    result = []\n",
    "    # Batch size depend on GPU memory\n",
    "    batch_size = 20\n",
    "    if len(match_list) % batch_size != 0:\n",
    "        add_batch = 1\n",
    "    else:\n",
    "        add_batch = 0\n",
    "    for i in range(len(match_list) // batch_size + add_batch):\n",
    "        with torch.no_grad():\n",
    "            inputs = get_inputs(match_list[i*batch_size: i*batch_size+batch_size], tokenizer)\n",
    "            inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "            inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "            scores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n",
    "            result.append(scores.detach().cpu())\n",
    "            torch.cuda.empty_cache()\n",
    "    result = torch.cat(result).tolist()\n",
    "    return result\n",
    "\n",
    "# Remove duplicate question\n",
    "def deduplicate(top_embedding_result):\n",
    "    query_count = {}\n",
    "    result = []\n",
    "    for qa_pair in top_embedding_result:\n",
    "        if qa_pair['User'] in query_count.keys():\n",
    "            query_count[qa_pair['User']] += 1\n",
    "        else:\n",
    "            query_count[qa_pair['User']] = 1\n",
    "            \n",
    "        if query_count[qa_pair['User']] <= 2:\n",
    "            result.append(qa_pair)\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Sample top p question\n",
    "def top_p_sampling(logits, p=0.8):\n",
    "    #sorted_indices = np.argsort(logits)  # Sort logits\n",
    "    sorted_probs = numpy.exp(logits) / numpy.sum(numpy.exp(logits))  # Convert sorted logits to probabilities\n",
    "    cum_probs = numpy.cumsum(sorted_probs)  # Calculate the cumulative probability\n",
    "    valid_indices = numpy.where(cum_probs <= (p))[0]  # Get valid indices where cumulative probability is above threshold\n",
    "    if len(valid_indices) > 0:\n",
    "        min_valid_index = valid_indices[-1]\n",
    "    else:\n",
    "        min_valid_index = len(logits)  # If no valid indices, select the last one (highest probability)\n",
    "    #selected_index = np.random.choice(mask)  # Randomly select an index from the valid set\n",
    "    return min(max(min_valid_index, 5), 20)\n",
    "\n",
    "# Get Rag content\n",
    "def llm_input(sample_result):\n",
    "    top = ''\n",
    "    botton = ''\n",
    "    for i in range(len(sample_result)):\n",
    "        if i % 2 == 0:\n",
    "            top = top +  ' User: ' + sample_result[i]['User'] + ' Gura: ' +  sample_result[i]['Gura']\n",
    "        if i % 2 == 1:\n",
    "            botton = ' User: ' + sample_result[i]['User'] + ' Gura: ' + sample_result[i]['Gura'] + botton\n",
    "        \n",
    "    return (top + botton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b966a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "search_record = []\n",
    "for i in range(len(user_input)):\n",
    "    print(i)\n",
    "    question_embedding = user_input_embedding[i]\n",
    "    similarity = question_embedding @ Query_embedding.T\n",
    "    \n",
    "    search_result = []\n",
    "    for j in range(len(similarity)):\n",
    "        qa_dataset[j]['embedding_score'] = float(similarity[j])\n",
    "        search_result.append(copy.copy(qa_dataset[j]))\n",
    "    \n",
    "    search_result.sort(key = lambda x: x['embedding_score'], reverse = True)\n",
    "    top_embedding_result = deduplicate(search_result[:200])\n",
    "   \n",
    "    match_list = [[user_input[i], text['User'] + text['Gura']] for text in top_embedding_result]\n",
    "    scores = compute_score(tokenizer,model, match_list)\n",
    "    #scores = similarity\n",
    "    for k in range(len(top_embedding_result)):\n",
    "        top_embedding_result[k]['rank_score'] = scores[k]\n",
    "\n",
    "    top_rank_result = copy.copy(top_embedding_result)\n",
    "    top_rank_result.sort(key = lambda x: x['rank_score'], reverse = True)\n",
    "    \n",
    "    rank_score = []\n",
    "    for l in range(len(top_rank_result)):\n",
    "        rank_score.append(top_rank_result[l]['rank_score'])\n",
    "    sample_result = top_rank_result[:top_p_sampling(rank_score, p=0.8)]\n",
    "    rag_content = llm_input(sample_result)\n",
    "    \n",
    "    search_record.append({'input_id':i, 'input_text': user_input[i], 'embedding_search_result': top_embedding_result[:40], \n",
    "                          'rank_search_result':top_rank_result[:10], 'rag_content': rag_content })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f7aad1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_id': 0,\n",
       " 'input_text': 'What is your favorite food?',\n",
       " 'embedding_search_result': [{'User': 'What is your favorite food?',\n",
       "   'Gura': 'I love all kinds of food, but I especially love fish! And spanakopita, that spinach puff stuff, yummy!',\n",
       "   'embedding_score': 1.0,\n",
       "   'rank_score': 6.22265625},\n",
       "  {'User': 'What is your favorite food?',\n",
       "   'Gura': 'I like all kinds of food, but I especially love shrimp!',\n",
       "   'embedding_score': 1.0,\n",
       "   'rank_score': 5.6171875},\n",
       "  {'User': \"What's your favorite food?\",\n",
       "   'Gura': 'I love all kinds of food, but I especially love fish!',\n",
       "   'embedding_score': 0.99853515625,\n",
       "   'rank_score': 5.71875},\n",
       "  {'User': \"What's your favorite food?\",\n",
       "   'Gura': 'I love all kinds of food, but I especially love fish!',\n",
       "   'embedding_score': 0.99853515625,\n",
       "   'rank_score': 5.71875}],\n",
       " 'rank_search_result': [{'User': 'What is your favorite food?',\n",
       "   'Gura': 'I love all kinds of food, but I especially love fish! And spanakopita, that spinach puff stuff, yummy!',\n",
       "   'embedding_score': 1.0,\n",
       "   'rank_score': 6.22265625},\n",
       "  {'User': \"What's your favorite food?\",\n",
       "   'Gura': 'I love all kinds of food, but I especially love fish!',\n",
       "   'embedding_score': 0.99853515625,\n",
       "   'rank_score': 5.71875},\n",
       "  {'User': \"What's your favorite food?\",\n",
       "   'Gura': 'I love all kinds of food, but I especially love fish!',\n",
       "   'embedding_score': 0.99853515625,\n",
       "   'rank_score': 5.71875},\n",
       "  {'User': 'What is your favorite food?',\n",
       "   'Gura': 'I like all kinds of food, but I especially love shrimp!',\n",
       "   'embedding_score': 1.0,\n",
       "   'rank_score': 5.6171875}],\n",
       " 'rag_content': \" User: What is your favorite food? Gura: I love all kinds of food, but I especially love fish! And spanakopita, that spinach puff stuff, yummy! User: What's your favorite food? Gura: I love all kinds of food, but I especially love fish! User: What is your favorite food? Gura: I like all kinds of food, but I especially love shrimp! User: What's your favorite food? Gura: I love all kinds of food, but I especially love fish!\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_record[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fintune-env",
   "language": "python",
   "name": "fintune-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
